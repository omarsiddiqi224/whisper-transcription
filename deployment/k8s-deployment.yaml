# deployment/k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper-backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: whisper-backend
  template:
    metadata:
      labels:
        app: whisper-backend
    spec:
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-l4  # Using L4 GPU
      containers:
      - name: whisper
        image: REGION-docker.pkg.dev/PROJECT_ID/whisper-repo/backend:latest
        resources:
          requests:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: 1
          limits:
            memory: "16Gi"
            cpu: "8"
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 9090
          name: websocket
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: whisper-secrets
              key: hf-token
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: whisper-secrets
              key: openai-key
        - name: DEVICE
          value: "cuda"
        - name: WHISPER_MODEL
          value: "small"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: whisper-frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: whisper-frontend
  template:
    metadata:
      labels:
        app: whisper-frontend
    spec:
      containers:
      - name: frontend
        image: REGION-docker.pkg.dev/PROJECT_ID/whisper-repo/frontend:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"