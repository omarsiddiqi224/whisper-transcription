# deployment/Dockerfile.backend
# Fixed version with proper WhisperX installation and GPU support

FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    python3-dev \
    ffmpeg \
    git \
    wget \
    curl \
    build-essential \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Upgrade pip
RUN pip3 install --upgrade pip setuptools wheel

# Install PyTorch with CUDA support first
RUN pip3 install torch==2.1.0+cu121 torchaudio==2.1.0+cu121 --index-url https://download.pytorch.org/whl/cu121

# Install core dependencies
RUN pip3 install --no-cache-dir \
    fastapi==0.104.1 \
    uvicorn[standard]==0.24.0 \
    python-multipart==0.0.6 \
    websockets==12.0 \
    numpy==1.24.3 \
    pandas==2.0.3 \
    ffmpeg-python==0.2.0 \
    openai==1.3.0

# Install Whisper and audio processing libraries
RUN pip3 install --no-cache-dir \
    faster-whisper==0.10.0 \
    pyannote.audio==3.1.1

# Install WhisperX from GitHub (main branch for latest fixes)
RUN pip3 install git+https://github.com/m-bain/whisperX.git@main --no-deps
RUN pip3 install nltk

# Copy application code
COPY backend/requirements.txt .
COPY backend/*.py ./

# Download NLTK data for sentence tokenization (used by WhisperX)
RUN python3 -c "import nltk; nltk.download('punkt')"

# Pre-download smaller model for faster startup (large-v3 will download on first use)
RUN python3 -c "from faster_whisper import WhisperModel; print('Testing imports...')"

# Expose ports
EXPOSE 8000 9090

# Environment variables for GPU
ENV CUDA_VISIBLE_DEVICES=0
ENV WHISPER_MODEL=large-v3
ENV DEVICE=cuda
ENV COMPUTE_TYPE=float16
ENV LLM_MODEL=gpt-4o-mini

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# Run the server
CMD ["python3", "-u", "backend_server.py"]